# -*- coding: utf-8 -*-
"""Job_recommendation_engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wpJhgG1pzGk1IneoGLLQrn123uiYdnSB

# Job recommendation engine

* Which country, state and city are popular among job creator?
* Which country, state and city are popular among job seekers?
* Recommend similar jobs based on the jobs title, description
* Recommend jobs based on similar user profiles

## Import dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import ast
from scipy import stats
from ast import literal_eval
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
# from nltk.stem.snowball import SnowballStemmer
# from nltk.stem.wordnet import WordNetLemmatizer
# from nltk.corpus import wordnet
# from surprise import Reader, Dataset, SVD, evaluate

import warnings; warnings.simplefilter('ignore')

import pandas as pd
import glob
import os

# Specifying the path to the input_data directory
input_data_path = "C:/Users/karth/OneDrive/Desktop/New folder/input_data/"

# Listing all TSV files in the input_data directory
for file in glob.glob(os.path.join(input_data_path, '*.tsv')):
    print(os.path.abspath(file))

# Loading datasets
apps = pd.read_csv(os.path.join(input_data_path, 'apps.tsv'), delimiter='\t', encoding='utf-8')
user_history = pd.read_csv(os.path.join(input_data_path, 'user_history.tsv'), delimiter='\t', encoding='utf-8')
jobs = pd.read_csv(os.path.join(input_data_path, 'jobs.tsv'), delimiter='\t', encoding='utf-8', on_bad_lines='skip')
users = pd.read_csv(os.path.join(input_data_path, 'users.tsv'), delimiter='\t', encoding='utf-8')
test_users = pd.read_csv(os.path.join(input_data_path, 'test_users.tsv'), delimiter='\t', encoding='utf-8')


apps.head()

apps.columns

apps.shape

apps.info()

user_history.head()

user_history.columns

user_history.shape

user_history.info()

jobs.head()

jobs.columns

jobs.shape

jobs.info()

users.head()

users.columns

users.shape

users.info()

test_users.head()

test_users.columns

test_users.shape

test_users.info()

"""## Exploratory Data Analysis (EDA) and Pre-processing

### Split training and testing data based on column `split`

* Here, there are three datafiles/dataframes are having attribute split.
    * apps
    * user_history
    * users
* This data attribute indicates that whether the data record can be used for training or testing so we need to filter out based on that.
* We are generating training and testing dataframes
"""

apps_training = apps.loc[apps['Split'] == 'Train']

apps_training.shape

apps_training.head()

apps_testing = apps.loc[apps['Split'] == 'Test']

apps_testing.shape

apps_testing.head()

user_history_training = user_history.loc[user_history['Split'] =='Train']

user_history_training = user_history.loc[user_history['Split'] =='Train']
user_history_testing = user_history.loc[user_history['Split'] =='Test']
apps_training = apps.loc[apps['Split'] == 'Train']
apps_testing = apps.loc[apps['Split'] == 'Test']
users_training = users.loc[users['Split']=='Train']
users_testing = users.loc[users['Split']=='Test']

user_history_training.shape

user_history_training.head()

user_history_testing = user_history.loc[user_history['Split'] =='Test']

user_history_testing.shape

user_history_testing.head()

users_training = users.loc[users['Split']=='Train']

users_training.shape

users_training.head()

users_testing = users.loc[users['Split']=='Test']

users_testing.shape

users_testing.head()

"""### List down all training data records"""

apps_training.head()

user_history_training.head()

users_training.head(5).transpose()

jobs.head()

"""### EDA for job openings based on their location information"""

jobs.groupby(['City','State','Country']).size().reset_index(name='Locationwise')

"""* Here note that for data attribute state, there are some blank/empty values are present.
* We will take care of it very soon
"""

jobs.groupby(['Country']).size().reset_index(name='Locationwise').sort_values('Locationwise',
                                                                             ascending=False).head()

Country_wise_job = jobs.groupby(['Country']).size().reset_index(name='Locationwise').sort_values('Locationwise',
                                                                             ascending=False)

plt.figure(figsize=(12,12))
ax = sns.barplot(x="Country", y="Locationwise", data=Country_wise_job)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="right")
ax.set_title('Country wise job openings')
plt.tight_layout()
plt.show()

"""### pre-processing

* Now we will perform following pre-processing steps:
  * We will consider only US region for building this recommendation engine
  * We will be removing data records where state is blank or state data attribute is having numerical value.(If needed)
"""

jobs_US = jobs.loc[jobs['Country']=='US']

jobs_US[['City','State','Country']]

jobs_US.groupby(['City','State','Country']).size().reset_index(name='Locationwise').sort_values('Locationwise',
                                                                             ascending=False).head()

State_wise_job_US = jobs_US.groupby(['State']).size().reset_index(name=
                                                                       'Locationwise'
                                                                      ).sort_values('Locationwise',ascending=False)

State_wise_job_US

plt.figure(figsize=(12,12))
ax = sns.barplot(x="State", y="Locationwise",data=State_wise_job_US)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="right")
ax.set_title('State wise job openings')
plt.tight_layout()
plt.show()

jobs_US.groupby(['City']).size().reset_index(name='Locationwise').sort_values('Locationwise',ascending=False)

City_wise_location = jobs_US.groupby(['City']).size().reset_index(
    name='Locationwise').sort_values('Locationwise',ascending=False)

City_wise_location_th = City_wise_location.loc[City_wise_location['Locationwise']>=12]

City_wise_location_th

plt.figure(figsize=(12,12))
ax = sns.barplot(x="City", y="Locationwise",data=City_wise_location_th.head(50))
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="right")
ax.set_title('City wise job openings')
plt.tight_layout()
plt.show()

"""#### Insights:

* When we do analysis state-wise then CA, TX, FL, IL and NY are having more job opening then other state
* When we do analysis city-wise then Houston, New York, Chicago, Dallas, Atlanta and Phoenix are having more jobs     compare to other cities

We have analyze the demand (job openings). Now there is time to analyze the supply(user-profiles)

### EDA for User profiles based on their location information
"""

users_training.groupby(['Country']).size().reset_index(name='Locationwise').sort_values('Locationwise',
                                                                                        ascending=False).head()

user_training_US = users_training.loc[users_training['Country']=='US']

user_training_US.shape

user_training_US.groupby(['State']).size().reset_index(
    name='Locationwise_state').sort_values('Locationwise_state',ascending=False)

user_training_US_state_wise = user_training_US.groupby(['State']).size().reset_index(
    name='Locationwise_state').sort_values('Locationwise_state',ascending=False)

user_training_US_th = user_training_US_state_wise.loc[user_training_US_state_wise['Locationwise_state']>=12]

plt.figure(figsize=(12,12))
ax = sns.barplot(x="State", y="Locationwise_state",data=user_training_US_th.head(50))
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="right")
ax.set_title('State wise job seekers')
plt.tight_layout()
plt.show()

user_training_US.groupby(['City']).size().reset_index(
    name='Locationwise_city').sort_values('Locationwise_city',ascending=False)

user_training_US_city_wise = user_training_US.groupby(['City']).size().reset_index(
    name='Locationwise_city').sort_values('Locationwise_city',ascending=False)

user_training_US_City_th = user_training_US_city_wise.loc[user_training_US_city_wise['Locationwise_city']>=12]

plt.figure(figsize=(12,12))
ax = sns.barplot(x="City", y="Locationwise_city",data=user_training_US_City_th.head(50))
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="right")
ax.set_title('State wise job seekers')
plt.tight_layout()
plt.show()

"""* Here note that we are going to consider the US region for our further analysis. We are not removing any
  data records right now.

* Up-till now we have obtain the popular state and city for job seeker and job creator

## revised approach


###  Let's find out Similar jobs
"""

jobs_US.columns

jobs_US.head().transpose()

jobs_US_base_line = jobs_US.iloc[0:10000,0:8]

jobs_US_base_line.head()

jobs_US_base_line['Title'] = jobs_US_base_line['Title'].fillna('')
jobs_US_base_line['Description'] = jobs_US_base_line['Description'].fillna('')
#jobs_US_base_line['Requirements'] = jobs_US_base_line['Requirements'].fillna('')

jobs_US_base_line['Description'] = jobs_US_base_line['Title'] + jobs_US_base_line['Description']

tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(jobs_US_base_line['Description'])

tfidf_matrix.shape

# http://scikit-learn.org/stable/modules/metrics.html#linear-kernel
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

cosine_sim[0]

jobs_US_base_line = jobs_US_base_line.reset_index()
titles = jobs_US_base_line['Title']
indices = pd.Series(jobs_US_base_line.index, index=jobs_US_base_line['Title'])
#indices.head(2)

def get_recommendations(title):
    idx = indices[title]
    #print (idx)
    sim_scores = list(enumerate(cosine_sim[idx]))
    #print (sim_scores)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    job_indices = [i[0] for i in sim_scores]
    return titles.iloc[job_indices]

get_recommendations('SAP Business Analyst / WM').head(10)

get_recommendations('Security Engineer/Technical Lead').head(10)

get_recommendations('Immediate Opening').head(10)

get_recommendations('EXPERIENCED ROOFERS').head(10)

"""## Best approach

#### Find out similar users -- Find out for which jobs they have applied -- suggest those job to the other users who shared similar user profile.

We are finding put similar user profile based on their degree type, majors and total years of experience.
* We will get to 10 similar users.
* We will find our which are the jobs for which these users have applied
* We take an union of these jobs and recommend the jobs all these user base
"""

users_training.head()

user_based_approach_US = users_training.loc[users_training['Country']=='US']

user_based_approach = user_based_approach_US.iloc[0:10000,:]

user_based_approach.head()

user_based_approach['DegreeType'] = user_based_approach['DegreeType'].fillna('')
user_based_approach['Major'] = user_based_approach['Major'].fillna('')
user_based_approach['TotalYearsExperience'] = user_based_approach['TotalYearsExperience'].fillna('').astype(str)

user_based_approach['DegreeType'] = (user_based_approach['DegreeType'] +
                                    user_based_approach['Major'] +
                                    user_based_approach['TotalYearsExperience'])

tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(user_based_approach['DegreeType'])

tfidf_matrix.shape

# http://scikit-learn.org/stable/modules/metrics.html#linear-kernel
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

cosine_sim[0]

user_based_approach = user_based_approach.reset_index()
userid = user_based_approach['UserID']
indices = pd.Series(user_based_approach.index, index=user_based_approach['UserID'])
#indices.head(2)

def get_recommendations_userwise(userid):
    idx = indices[userid]
    #print (idx)
    sim_scores = list(enumerate(cosine_sim[idx]))
    #print (sim_scores)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    user_indices = [i[0] for i in sim_scores]
    #print (user_indices)
    return user_indices[0:11]

print ("-----Top 10 Similar users with userId: 123------")
get_recommendations_userwise(123)

def get_job_id(usrid_list):
    jobs_userwise = apps_training['UserID'].isin(usrid_list) #
    df1 = pd.DataFrame(data = apps_training[jobs_userwise], columns=['JobID'])
    joblist = df1['JobID'].tolist()
    Job_list = jobs['JobID'].isin(joblist) #[1083186, 516837, 507614, 754917, 686406, 1058896, 335132])
    df_temp = pd.DataFrame(data = jobs[Job_list], columns=['JobID','Title','Description','City','State'])
    return df_temp

get_job_id(get_recommendations_userwise(123))

print ("-----Top 10 Similar users with userId: 47------")
get_recommendations_userwise(47)

get_job_id(get_recommendations_userwise(47))

